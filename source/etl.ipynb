{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f3050f",
   "metadata": {},
   "source": [
    "### Data preparation and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# @file: etl.py\n",
    "# @authors: \n",
    "#   Ricardo Fernández  - A01704813\n",
    "#   Arturo Díaz        - A01709522\n",
    "#   Yuna Chung         - A01709043\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# @brief: Reads the content of a file given its filename\n",
    "# @param filename: Name of the file to read\n",
    "# @return: Content of the file as a string\n",
    "# --------------------------------------------------------------\n",
    "def read_file_content(filename, base_path):\n",
    "    file_path = os.path.join(base_path, f\"{filename}.java\")\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"\"\n",
    "    \n",
    "# --------------------------------------------------------------\n",
    "# @brief: Gets the first code file found in a directory\n",
    "# @param path: Path to the directory to search\n",
    "# @return: Full path to the first Java file found, or None if not found\n",
    "# --------------------------------------------------------------\n",
    "def get_code_file_from_dir(path):\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".java\"):\n",
    "            return os.path.join(path, file)\n",
    "    return None\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# @brief: Cleans a given text by removing all whitespace characters\n",
    "# @param text: Text to clean\n",
    "# @return: Cleaned text with no whitespace characters\n",
    "# --------------------------------------------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace(\"\\t\", \"\")\n",
    "    text = text.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\" {2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# @brief: Extracts data from the CONPLAG_Version_2 Dataset\n",
    "# @param base_path: Base path where the dataset is located\n",
    "# @return: DataFrame with the extracted data\n",
    "# --------------------------------------------------------------\n",
    "def extractConplag(base_path=\"../dataset/CONPLAG_VERSION2/versions\"):\n",
    "    labels_df = pd.read_csv(os.path.join(base_path, \"labels.csv\"))\n",
    "    df = labels_df[[\"sub1\", \"sub2\", \"verdict\"]].copy()\n",
    "    df.columns = [\"sub1\", \"sub2\", \"Plagiarized\"]\n",
    "    df[\"Plagiarized\"] = df[\"Plagiarized\"].astype(int)\n",
    "\n",
    "    contents1 = []\n",
    "    contents2 = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sub1 = row[\"sub1\"]\n",
    "        sub2 = row[\"sub2\"]\n",
    "        folder_name = f\"{sub1}_{sub2}\"\n",
    "\n",
    "        # Paths\n",
    "        version1_file = os.path.join(base_path, \"version_1\", folder_name, f\"{sub1}.java\")\n",
    "        version2_file = os.path.join(base_path, \"version_2\", folder_name, f\"{sub2}.java\")\n",
    "\n",
    "        # Leer archivos\n",
    "        content1 = \"\"\n",
    "        content2 = \"\"\n",
    "        try:\n",
    "            with open(version1_file, \"r\", encoding=\"utf-8\") as f1:\n",
    "                content1 = clean_text(f1.read())\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARN] Archivo no encontrado: {version1_file}\")\n",
    "        \n",
    "        try:\n",
    "            with open(version2_file, \"r\", encoding=\"utf-8\") as f2:\n",
    "                content2 = clean_text(f2.read())\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARN] Archivo no encontrado: {version2_file}\")\n",
    "\n",
    "        contents1.append(content1)\n",
    "        contents2.append(content2)\n",
    "\n",
    "    df[\"File1\"] = contents1\n",
    "    df[\"File2\"] = contents2\n",
    "\n",
    "    return df[[\"File1\", \"File2\", \"Plagiarized\"]]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# @brief: Extacts data from the FIRE14 Dataset\n",
    "# @param qrel_path: Path to the qrel file\n",
    "# @param base_path: Base path where the Java files are located\n",
    "# @return: DataFrame with the extracted data\n",
    "# --------------------------------------------------------------\n",
    "def extractFire14(base_path=\"../dataset/FIRE14\"):\n",
    "    qrel_path = os.path.join(base_path, \"SOCO14-java.qrel\")\n",
    "    java_path = os.path.join(base_path, \"java\")\n",
    "\n",
    "    # Leer pares plagiados (qrel)\n",
    "    plag_pairs = []\n",
    "    with open(qrel_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            f1, f2 = line.strip().split()\n",
    "            plag_pairs.append((f1, f2))\n",
    "\n",
    "    # Lista de todos los archivos .java\n",
    "    all_files = [f for f in os.listdir(java_path) if f.endswith(\".java\")]\n",
    "\n",
    "    # Crear DataFrame para pares positivos (plagio)\n",
    "    data = []\n",
    "    for f1, f2 in plag_pairs:\n",
    "        content1 = clean_text(read_file_content(f1.replace(\".java\", \"\"), java_path))\n",
    "        content2 = clean_text(read_file_content(f2.replace(\".java\", \"\"), java_path))\n",
    "        data.append({\"File1\": content1, \"File2\": content2, \"Plagiarized\": 1})\n",
    "\n",
    "    # Crear pares negativos (no plagio) - pares no listados en plag_pairs\n",
    "    plag_set = set(plag_pairs)\n",
    "    # Generar combinaciones de pares (sin ordenar)\n",
    "    for i in range(len(all_files)):\n",
    "        for j in range(i + 1, len(all_files)):\n",
    "            f1 = all_files[i]\n",
    "            f2 = all_files[j]\n",
    "            pair = (f1, f2)\n",
    "            if pair not in plag_set and (f2, f1) not in plag_set:\n",
    "                content1 = clean_text(read_file_content(f1.replace(\".java\", \"\"), java_path))\n",
    "                content2 = clean_text(read_file_content(f2.replace(\".java\", \"\"), java_path))\n",
    "                data.append({\"File1\": content1, \"File2\": content2, \"Plagiarized\": 0})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# @brief: Extacts data from the IRPLAG Dataset\n",
    "# @param base_path: Base path where the IR-Plag Dataset is located\n",
    "# @return: DataFrame with the extracted data\n",
    "# --------------------------------------------------------------\n",
    "def extractIRPlag(base_path=\"../dataset/IRPLAG\"):\n",
    "    data = []\n",
    "\n",
    "    # Iterar sobre cada \"case-*\" folder\n",
    "    for case_dir in os.listdir(base_path):\n",
    "        case_path = os.path.join(base_path, case_dir)\n",
    "        if not os.path.isdir(case_path):\n",
    "            continue\n",
    "\n",
    "        # Obtener archivo original (único archivo dentro de /original)\n",
    "        original_dir = os.path.join(case_path, \"original\")\n",
    "        original_file_path = get_code_file_from_dir(original_dir)\n",
    "        if not original_file_path:\n",
    "            continue\n",
    "        with open(original_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            original_content = f.read()\n",
    "        original_content = clean_text(original_content)\n",
    "\n",
    "        # Agregar pares con plagio\n",
    "        plag_base = os.path.join(case_path, \"plagiarized\")\n",
    "        for level in os.listdir(plag_base):\n",
    "            level_path = os.path.join(plag_base, level)\n",
    "            for instance in os.listdir(level_path):\n",
    "                file_path = get_code_file_from_dir(os.path.join(level_path, instance))\n",
    "                if not file_path:\n",
    "                    continue\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    plag_content = f.read()\n",
    "                plag_content = clean_text(plag_content)\n",
    "                data.append({\n",
    "                    \"File1\": original_content,\n",
    "                    \"File2\": plag_content,\n",
    "                    \"Plagiarized\": 1\n",
    "                })\n",
    "\n",
    "        # Agregar pares sin plagio\n",
    "        non_plag_base = os.path.join(case_path, \"non-plagiarized\")\n",
    "        for instance in os.listdir(non_plag_base):\n",
    "            file_path = get_code_file_from_dir(os.path.join(non_plag_base, instance))\n",
    "            if not file_path:\n",
    "                continue\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                non_plag_content = f.read()\n",
    "            non_plag_content = clean_text(non_plag_content)\n",
    "            data.append({\n",
    "                \"File1\": original_content,\n",
    "                \"File2\": non_plag_content,\n",
    "                \"Plagiarized\": 0\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "741e2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_datasets():\n",
    "    # Obtengo los datasets\n",
    "    df_conplag = extractConplag()\n",
    "    df_fire14 = extractFire14()\n",
    "    df_irplag = extractIRPlag()\n",
    "\n",
    "    # Agrego una columna para identificar la fuente\n",
    "    df_conplag[\"Source\"] = \"Conplag\"\n",
    "    df_fire14[\"Source\"] = \"Fire14\"\n",
    "    df_irplag[\"Source\"] = \"IRPlag\"\n",
    "\n",
    "    # Uno los DataFrames\n",
    "    final_df = pd.concat([df_conplag, df_fire14, df_irplag], ignore_index=True)\n",
    "\n",
    "    # Mezclo las filas aleatoriamente\n",
    "    final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ea8a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in merged dataset: 34782\n"
     ]
    }
   ],
   "source": [
    "df = merge_all_datasets()\n",
    "print(f\"Total rows in merged dataset: {len(df)}\")\n",
    "\n",
    "# Exporto el DataFrame a CSV, JSON y Parquet\n",
    "df.to_csv(\"../dataset/dataset.csv\", index=False)\n",
    "df.to_json(\"../dataset/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "df.to_parquet(\"../dataset/dataset.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
